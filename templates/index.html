<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>YOLO26 Segmentation Video Streams</title>
    <style>
      body {
        display: flex;
        justify-content: space-around;
        align-items: center;
        height: 100vh;
        margin: 0;
        background-color: #f0f0f0;
      }
      img {
        max-width: 45%;
        height: auto;
      }
    </style>
  </head>
  <body>
    <h1>Real-time YOLO26 Segmentation (WebRTC)</h1>

    <div
      id="calibrationWarning"
      style="
        display: none;
        margin-bottom: 20px;
        padding: 15px;
        border: 2px solid #ff4444;
        background: #ffe6e6;
        color: #cc0000;
        font-weight: bold;
      "
    >
      System Uncalibrated! Homography matrix not found. Object fusion will not
      work.
      <button
        onclick="location.href='/calibrate'"
        style="
          margin-left: 20px;
          padding: 5px 15px;
          background: #ff4444;
          color: white;
          border: none;
          cursor: pointer;
        "
      >
        Go to Calibration
      </button>
    </div>

    <div
      id="controlPanel"
      style="
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ccc;
        background: #fff;
      "
    >
      <button id="triggerBtn" style="padding: 10px 20px; cursor: pointer">
        Start Capturing Best Result (1 min)
      </button>
      <span
        id="triggerStatus"
        style="margin-left: 20px; font-weight: bold"
      ></span>
    </div>

    <div style="display: flex; gap: 20px; align-items: flex-start">
      <div>
        <video
          id="video1"
          autoplay
          playsinline
          muted
          style="width: 480px; height: auto; border: 1px solid #333"
        ></video>
        <div>Camera 1 (WebRTC)</div>
      </div>
      <div>
        <video
          id="video2"
          autoplay
          playsinline
          muted
          style="width: 480px; height: auto; border: 1px solid #333"
        ></video>
        <div>Camera 2 (WebRTC)</div>
      </div>
    </div>

    <script>
      async function startWebRTC(cam, videoEl, mode = 'pipeline') {
        // fetch ICE config from server (supports TURN if configured)
        let iceServers = [{ urls: "stun:stun.l.google.com:19302" }];
        try {
          const r = await fetch("/ice");
          if (r.ok) {
            const j = await r.json();
            if (j && j.iceServers) iceServers = j.iceServers;
          }
        } catch (e) {
          console.warn("Failed to fetch /ice, using default STUN", e);
        }
        const pc = new RTCPeerConnection({ iceServers });
        pc.ontrack = (evt) => {
          videoEl.srcObject = evt.streams[0];
        };

        pc.oniceconnectionstatechange = () =>
          console.log("ICE", pc.iceConnectionState);

        // indicate we want to receive video so the offer includes media m= lines
        pc.addTransceiver("video", { direction: "recvonly" });
        const offer = await pc.createOffer();
        await pc.setLocalDescription(offer);

        const resp = await fetch(
          `http://${location.hostname}:8080/offer?cam=${cam}&mode=${mode}`,
          {
            method: "POST",
            body: JSON.stringify({ sdp: offer.sdp, type: offer.type }),
            headers: { "Content-Type": "application/json" },
          },
        );
        const ans = await resp.json();
        await pc.setRemoteDescription(new RTCSessionDescription(ans));
      }

      // Simplified: the detection/capture page does not auto-start WebRTC streams.
      // Users should open the dedicated livestream page for raw/annotated live video.
      window.addEventListener("DOMContentLoaded", () => {
        const v1 = document.getElementById("video1");
        const v2 = document.getElementById("video2");
        const btn = document.getElementById("triggerBtn");
        const status = document.getElementById("triggerStatus");

        let pollingActive = false;
        let streamsStarted = false;

        const tryStart = async (cam, videoEl, attempts = 3, mode = 'pipeline') => {
          for (let i = 0; i < attempts; i++) {
            try {
              // explicitly request pipeline annotated mode from server
              await startWebRTC(cam, videoEl, mode);
              console.log("WebRTC started for cam", cam);
              return;
            } catch (err) {
              console.warn("Failed to start cam", cam, err);
              await new Promise((r) => setTimeout(r, 500 * (i + 1)));
            }
          }
          console.error("Giving up starting cam", cam);
        };

        async function startStreamsIfNeeded() {
          if (streamsStarted) return;
          streamsStarted = true;
          tryStart(1, v1, 3, 'pipeline');
          tryStart(2, v2, 3, 'pipeline');
        }

        async function updateStatus() {
          try {
            const r = await fetch("/trigger/status");
            if (r.ok) {
              const j = await r.json();

              const warning = document.getElementById("calibrationWarning");
              const controlPanel = document.getElementById("controlPanel");

              if (!j.has_homography) {
                warning.style.display = "block";
                controlPanel.style.display = "none";
              } else {
                warning.style.display = "none";
                controlPanel.style.display = "block";
              }

              if (j.active) {
                status.innerText = `Active! Time left: ${Math.round(j.time_left)}s | Best result so far: ${j.best_count} objects`;
                btn.disabled = true;
                // If a capture session is active, show annotated streams
                startStreamsIfNeeded();
                setTimeout(updateStatus, 1000);
              } else {
                if (j.best_count > 0) {
                  status.innerText = `Idle (Last best result: ${j.best_count} objects)`;
                } else {
                  status.innerText = "Idle";
                }
                btn.disabled = !j.has_homography;
                pollingActive = false;
              }
            }
          } catch (e) {
            console.error("Polling error", e);
            pollingActive = false;
          }
        }

        // Initial check
        updateStatus();
        setInterval(updateStatus, 5000);
        
        // Always start WebRTC streams on page load (show raw video by default)
        startStreamsIfNeeded();

        btn.onclick = async () => {
          btn.disabled = true;
          const resp = await fetch("/trigger", {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({ duration: 60 }),
          });

          if (resp.ok && !pollingActive) {
            pollingActive = true;
            updateStatus();
          }

          // Start annotated WebRTC streams on button click
          startStreamsIfNeeded();
        };

        // Add quick link to open the Livestream page
        const openLivestream = document.createElement('button');
        openLivestream.innerText = 'Open Livestream';
        openLivestream.style = 'margin-left:20px; padding:8px 12px; cursor:pointer';
        openLivestream.onclick = () => window.open('/livestream', '_blank');
        document.getElementById('controlPanel').appendChild(openLivestream);

        // If a session was already active when the page loaded, ensure streams start
        (async () => {
          try {
            const r = await fetch('/trigger/status');
            if (r.ok) {
              const j = await r.json();
              if (j.active) {
                startStreamsIfNeeded();
              }
            }
          } catch (e) {
            console.warn('Failed initial session check', e);
          }
        })();
      });
    </script>
  </body>
</html>
